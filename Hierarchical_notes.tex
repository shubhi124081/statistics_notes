\documentclass[12pt,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Hierarchical modeling notes},
            pdfauthor={Shubhi Sharma},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Hierarchical modeling notes}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Shubhi Sharma}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{9/2/2019}

\usepackage{placeins}
\usepackage{color}
\usepackage{amsmath}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{3}
\tableofcontents
}
\section{1. Linear Algebra Review}\label{linear-algebra-review}

\section{2. Introduction to hierarchical
modeling}\label{introduction-to-hierarchical-modeling}

\section{3. Analysis of variance}\label{analysis-of-variance}

\subsection{3.1 Introduction to ANOVA}\label{introduction-to-anova}

Analysis of variance (ANOVA) refers to a specific set of methods for
data analysis and to a way of summarizing multilevel models. In
classical statistics, ANOVAs generally refer to either a family of
additive data decomposition or to a method of testing the statistical
significance of added predictors in a linear model.

As a tool for data analysis, ANOVA is typically used to learn the
relative importance of different sources of variation in a dataset. In
classical statistics, ANOVA refers either to a family of additive data
decompositions, or to a mehtod of testing the statistical significance
of added predictors in a linear model. If a multilevel model has already
been fit, it can be summarixed by the variation in each of its batches
of coefficients.

A model with treatment effects and categories may be specified as the
following,

\[y_i =  \mu + \gamma_{j[i]} + \delta_{k[i]} + \epsilon_i\] or
equivalently,

\[y_{jk} = \mu + \gamma_j + \delta_k + \epsilon_{jk}\] where i indexes
the number of observations, j indexes the different treatment effects
and k indexes categories.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#An example of anova code }
\CommentTok{#summary(aov(y ~ factor(treatment) + factor(category)))}
\end{Highlighting}
\end{Shaded}

In this case, the residuals are equivalent to the treatment x categories
interactions.

\textit{Explaining the output of an ANOVA table}

\textit{Sources of variation and degrees of freedom}. Degrees of freedom
are defined as the number of coefficients in that group, minus the
number of constraints required for the coefficients to be identifiable
in a classical regression.

\textit{Sums of squares}. Sums of squares are derived from the classical
coefficient estimates. Thus, the sums of squares for treatments,
categories and residuals are \(\sum_i^{n} \hat{\gamma_{j[i]}}^2\),
\(\sum_i^{n} \hat{\lambda_{j[i]}}^2\) and
\(\sum_i^{n} \hat{\epsilon_{j[i]}}^2\). If the data are balanced, the
sums of squares in the table add up to the ``total sum of squares'' of
the data \(\sum_i^n(y_i - \hat{\mu})^2\). Roughly speaking, in a
balanced design there are the same number of observations in each row
and each column of the data.

\textit{Mean squares, F ratios, and p-values}. For each source of
variation in the ANOVA table, the mean square is defined as the sum of
squares divided by the degrees of freedom. The ratios of the mean
squares are called the F statistics, and the usual goal of classical
ANOVA is to find F-ratios that are significantly greater than 1. The
p-values in an ANOVA table indicate the statistical signficance of the
F-tests.

(Model selection nested ANOVAs)

\subsection{3.2 ANOVA and multilevel linear and generalized linear
models}\label{anova-and-multilevel-linear-and-generalized-linear-models}

When moving to multilevel modeling, the key idea we want to take from
ANOVA is the estimation of the importance of different batches of
predictors (``components of variation''). In general, the goal is
estimation rather than testing. ANOVA applied to linear regression
models have the following set up,

\[y_i = \sum_{m=0}^M\sum_{j=1}^{J_m} X_{ij}^{(m)} \beta_j^{(m)} \] Note
that the essence of analysis of variance is in the structuring of the
coefficients into batches - hence the notation \(\beta_j^{(m)}\) - going
beyond the usual linear model formulation that has a single indexing of
coefficients \(\beta_j\).

Each batch of regression coefficients is assumed to be a sample from a
normal distribution with mean 0 and population standard deviation
\(\sigma_m\),

\[\beta_j^{(m)}\sim N(0, \sigma^2_m)\] The mean of 0 for each batch of
regression coefficients comes naturally from the ANOVA decomposition
structure (pulling out the grand mean, main effects, interactions and so
forth), the standard deviations represent the magnitudes of the variance
components corresponding to each row of the table.

\section{3.2 One-way ANOVA model}\label{one-way-anova-model}

Consider a model of the form,

\[
\begin{aligned}
1. y_{i,j} &= \mu + \alpha_j + \epsilon_{i,j}\\
2. y_{i,j} &= \mu_j + \epsilon_{i,j}\\
\end{aligned}
\] where \(\mu_j = \mu + \alpha_j\), (1) is the treatment effects model
and (2) is the treatment means model. These two models are just
reparameterizations of each other. In this model \(\mu_j\) represents
expected mean from group i, \(\mu\) represents expected mean across all
groups, \(\alpha_j\) represents deviation of the region specific
expected yield from the overall expectation and finally,
\(\epsilon_{i,j}\) represents deviations of the observed plot yield from
their region specific expectations.

For these terms to be interpretable, the standard ANOVA model
parameterizes things so that

\[
\begin{aligned}
1. \sum_j \alpha_j &= 0 \\
2. \{\epsilon_{i,j}\} &\sim p(\epsilon) \\
\end{aligned}
\]

The expected yield for an individual in group j is,

\[\mathbb{E}[y_{ij} \mid \mu, \alpha_1, \cdots, \alpha_m] = \mathbb{E}[\mu + \alpha_j + \epsilon_{ij}\mid \mu, \alpha_1, \cdots, \alpha_m] = \mu + \alpha_j = \mu_j\]

\subsection{3.2.1 Parameter estimation}\label{parameter-estimation}

The ordinary least squares estimate of \(\pmb{\mu}\) is the value that
minimizes the sum of squared residuals i.e.~the sum o fthe squared
errors in the model fit. Recall the OLS derivation,

An OLS estimates of \(\pmb{\hat{\mu}}\) contains the estimates of the
group means \(\pmb{\hat{\mu}} = \{\hat{\mu_1}, \cdots, \hat{\mu_M}\}\)
where M is the total number of groups. Considering just one group mean
\(\hat{\mu_j}\),

The OLS estimate of the group mean is the estimate that minimizes the
sum of squared error,
\[SSE(\hat{\mu_j}) = \sum_i^N(y_{i,j} - \hat{\mu_j})^2\]

Taking the derivative of \(SSE(\hat{\mu_j})\),

\[
\begin{aligned}
\frac{\partial}{\partial \mu_j} SSE(\hat{\mu_j}) &= \frac{\partial}{\partial{\mu_j}} \sum_i^N (y_{i,j} - \hat{\mu_j})^2 \\ 
&= \frac{\partial}{\partial \mu_j}\sum_i^N (y_{i,j}^2 - 2y_{i,j} \hat{\mu_j} + \hat{\mu_j}^2)\\
&= \sum_i^N(0 - 2y_{ij}+ 2\hat{\mu_j})\\
\text{setting the derivative to 0, }\\
-2\sum_i^N (y_{i,j}  - \hat{\mu}) &= 0 \\ 
\sum_i^N (y_{i,j})  - \sum_i^N\hat{\mu_j} &= 0\\ 
\text{Since  } n_j = N, \text{we can write }\sum_i^Ny_{i,j} = N\bar{y_j}, \\
N\bar{y_j} &= N\hat{\mu_j}\\ 
\text{Multiplying both sides by }&\frac{1}{N}, \\
\bar{y_j} &= \hat{\mu_j} \
\end{aligned}
\]

Therefore the vector of OLS means \(\pmb{\mu_{OLS}}\) will contain
\((\bar{y_1}, \cdots, \bar{y_m})\)

\textbf{B: Using the scalar formulation of the ANOVA model $\pmb{y_{ij} \sim N(\mu + \alpha_j, \sigma^2)}$ with the constraint $\pmb{\sum_j \alpha_j = 0}$, assume $\pmb{n_j = n}$ and show}
\textbf{i. $\pmb{\hat{\mu} = \bar{y_{..}}}$ is the grand mean over all observations}

The OLS estimate for \(\pmb{\hat{\mu}}\) minimizes the sum of squared
errors,

\[SSE(\hat{\mu})  = \sum_j^M\sum_i^N(y_{i,j} - \hat{\mu_j})^2\]

where, \(\hat{\mu_j} = \hat{\mu} + \hat{\alpha_j}\) \[
\begin{aligned}
SSE(\hat{\mu}) &= \sum_j^M \sum_i^N(y_{i,j} - \hat{\mu} - \hat{\alpha_j})^2 \\ 
\text{Taking a partial derivative wrt } \hat{\mu_j}\\ 
\frac{\partial}{\partial\hat{\mu}}SSE(\hat{\mu}) &= -2\sum_j^M \sum_i^N(y_{i,j} - \hat{\mu} - \hat{\alpha_j})\\
\text{Setting the derivative =0, }\\ 
-2\sum_j^M\sum_i^N(y_{i,j} - \hat{\mu} - \hat{\alpha_j}) &= 0\\
\sum_j^M\sum_i^N(y_{i,j} - \hat{\mu} - \hat{\alpha_j}) &= 0\\
\sum_j^M (\sum_i^Ny_{ij} -\sum_i^N\hat{\mu} - \sum_i^N\hat{\alpha_j}) &=0\\
\text{Since } n_j = N, \text{ we can write } \sum_i^Ny_{ij} = N\bar{y_j} \\
\sum_j^M((N\bar{y_j}) - N\hat{\mu} - N\hat{\alpha_j}) &= 0\\
\text{Multiplying every term by } \frac{1}{N},\\
\sum_j^M(\bar{y_j} - \hat{\mu} - \hat{\alpha_j}) &= 0\\
\sum_j^M(\bar{y_j}) - \sum_j^M\hat{\mu} - \sum_j^M\hat{\alpha_j} &= 0\\
\text{Using } \sum_j^M \hat{\alpha_j} = 0, \\
 M\bar{y_{..}} - M\hat{\mu} - 0 &= 0 \\
 M \bar{y_{..}} &= M\hat{\mu}\\
\text{Therefore, }\\
\hat{\mu} &= \bar{y_{..}}
\end{aligned}
\]

where M represents the number of groups, i.e \(j = \{1, \cdots, M\}\)

\textbf{ii. Show $\pmb{\hat{\mu} = \frac{1}{J}\sum_j\hat{\mu_j}}$}

From the given model specification, we know that
\(\mu_j = \mu + \alpha_j\)

\[
\begin{aligned}
\text{Taking RHS of equation ii, }\\
\frac{1}{J}\sum_j\hat{\mu_j} &= \frac{1}{J}\sum_j(\hat{\mu} + \hat{\alpha_j})\\
&= \frac{1}{J}(\sum_j\hat{\mu} + \sum_j\hat{\alpha_j})\\
&= \frac{1}{J}(J\hat{\mu}) + 0 \qquad \qquad \qquad \text{- since } \sum_j\hat{\alpha_j} = 0\\
&= \hat{\mu}
\end{aligned}
\]

Since LHS = RHS, we have \(\hat{\mu} = \frac{1}{J}\sum_j \hat{\mu_j}\).

\textbf{iii. Show $\pmb{\hat{\alpha_j}=\hat{\mu_j} - \hat{\mu}= \bar{y_j} - \bar{y_{..}}}$}

In order to show that
\(\hat{\alpha_j} = \hat{\mu_j} - \hat{\mu}= \bar{y_j} - \bar{y_{..}}\),
we have to first show that \(\hat{\mu_j} = \bar{y_j}\)

The OLS estimator for \(\mu_j\) would minimize \(SSE(\mu_j)\), \[
\begin{aligned}
\frac{\partial}{\partial\mu_j}SSE(\hat{\mu_j}) &= \frac{\partial}{\partial\mu_j}\sum_i^N(y_{ij} - \hat{\mu_j})^2\\
\text{Setting the derivative to zero, }\\
\frac{\partial}{\partial\mu_j}\sum_i^N(y_{ij} - \hat{\mu_j})^2 &= 0\\
2\sum_i^N(y_{ij} - \hat{\mu_j}) &= 0\\
\sum_i^N(y_{ij}) - \sum_i^N\hat{\mu_j} &= 0\\
\text{Since } n_j = N, \\
N\hat{\mu_j} &= N\bar{y_j}\\
\text{Multiplying both sides with }\frac{1}{N}, \\
\hat{\mu_j} &= \bar{y_j}\\
\end{aligned}
\]

Basically, a single data point can be ``decomposed'' in the following
way,

\[
\begin{aligned}
y_{ij} &= \bar{y_{..}} + (\bar{y_{.j}} - \bar{y_{..}}) + (y_{ij} - \bar{y_{.j}})\\
& \qquad \hat{\mu} + \qquad \hat{\alpha_j} \quad + \quad\hat{\epsilon_{i,j}}\\
\end{aligned}
\]

\textbf{Testing for across group variation}

The primary use of the ANOVA table is to evaluate if there are any group
effects, i.e.~to evaluate the competing hypotheses

\(H_0: \mu_{j_1} = \mu_{j_2}\) for all \(j_1, j_2\) versus
\(H_1: \mu_{j_1} \neq \mu_{j_2}\) for some \(j_1, j_2\).

The ANOVA table does this by comparing across-group variation to within
group variation via the F statistic = MSG/MSE.

If \(H_0\) is true \(MSE \approx \sigma^2\) and
\(MSG \approx \sigma^2\). If \(H_1\) is true, \(MSE \approx \sigma^2\)
and \(MSG \approx \sigma^2 + ns^2_a > \sigma^2\).

Basically MSG/MSE should be around 1 under null hypothesis and under the
alternate hypothesis, MSG/MSE should be bigger than 1.

The ANOVA model tells us something about variability within each of the
m regions and the variability across these m regions. A model for across
group heterogeneity is provided by the hierarchical normal model,

\[
\begin{aligned}
y_{i,j} &= \mu + \alpha_j + \epsilon_{i,j}\\
\alpha_1, \cdots, \alpha_m &\sim N(0, \tau^2)\\
\epsilon_{i,j} &\sim N(0, \sigma^2)\\
\end{aligned}
\]

\(\alpha_1, \cdots, \alpha_m\) and \(\epsilon_{ij}\) represent
differences in observations across groups and within groups,
respectively.

What is expectation, covariance and variance under this model?

Expectation \[
\begin{aligned}
\mathbb{E}[y_{i,j}] &= \mathbb{E}[\mu + \alpha_j + \epsilon_{ij}]\\
&= \mu + 0 + 0 \\
\mathbb{E}[y_{i,j}]&= \mu\\
\end{aligned}
\]

Variance

\[
\begin{aligned}
Var[y_{ij}] &= \mathbb{E}[(y_{ij} - \mathbb{E}[y_{ij}]^2]\\
&= \mathbb{E}[(y_{ij} - \mu)^2]\\
&= \mathbb{E}[(\mu + \alpha_j + \epsilon_{ij} - \mu)^2]\\
&= \mathbb{E}[(\alpha_j + \epsilon_{ij})^2]\\
&= \mathbb{E}[\alpha_j^2] + \mathbb{E}[2\alpha_j\epsilon_{ij}] + \mathbb{E}[\epsilon_{ij}^2]\\
&= \tau^2 +  2\times 0\times 0 + \sigma^2\\
Var[y_{ij}]&= \tau^2 + \sigma^2\\
\end{aligned}
\]

Covariance for two data points in the different groups

\[
\begin{aligned}
Cov[y_{ij}, y_{i^\prime, j^\prime}] &= \mathbb{E}[(y_{ij} - \mathbb{E}[y_{ij}])(y_{i^\prime j^\prime} - \mathbb{E}[y_{i^\prime, j^\prime}])]\\
&=\mathbb{E}[(y_{ij} - \mu)(y_{i^\prime j^\prime} - \mu)]\\
&= \mathbb{E}[y_{ij}y_{i^\prime j^\prime} - y_{ij}\mu  - y_{i^\prime j^\prime}\mu + \mu^2]\\
&= \mathbb[y_{ij}y_{i^\prime j^\prime} ] - \mu^2 - \mu^2 + \mu^2\\
&= \mu^2 - \mu^2 - \mu^2 + \mu^2\\
&= 0
\end{aligned}
\]

Covariance for two data points in the same group

\[
\begin{aligned}
Cov[y_{ij}, y_{i^\prime, j}] &= \mathbb{E}[(y_{ij} - \mathbb{E}[y_{ij}])(y_{i^\prime j } - \mathbb{E[y_{i^\prime j}]})]\\
&= \mathbb{E}[(y_{ij} - \mu)(y_{i^\prime j} - \mu)]\\
&= \mathbb{E}[(\mu + \alpha_j + \epsilon_{ij} - \mu)(\mu + \alpha_j + \epsilon_{i^\prime j} - \mu)]\\
&= \tau^2
\end{aligned}
\]

In matrix formulation

Expectation \[
\begin{aligned}
\mathbb{E}[\mathbf{y}_j] &= \mathbb{E}[\mathbf{X_j \beta + X_jb_j + \epsilon_j}]\\
&= \mathbf{X_j \beta} + X_j0 + 0\\
&= \mathbf{X_j \beta}\\
\end{aligned}
\]

Covariance

\section{4. Estimation}\label{estimation}

\section{5. Bayesian methods}\label{bayesian-methods}

\section{6. Summary on ANOVA models}\label{summary-on-anova-models}

\section{7. Multilevel Linear Models:
Introduction}\label{multilevel-linear-models-introduction}

\subsection{7.1 Types of multilevel
models}\label{types-of-multilevel-models}

\emph{Notes from Gelman and Hill (2006), Chapter 11}

Multilevel models are extensions of regression in which data are
structured in groups and coefficients can vary by group.

With grouped data, a regression that includes indicators for groups is
called a \textbf{varying-intercept model} becuase it can be interpreted
as a model with a different intercept within each group. This model can
be written with indicators,

\[
Y_{j[i]} = \mathbb{I}_{j = 1}\alpha_1 + \mathbb{I}_{j = 2}\alpha_2 + \mathbb{I}_{j = 3}\alpha_3 + \mathbb{I}_{j = 4}\alpha_4 + \mathbb{I}_{j = 5}\alpha_5 +\beta X_{j[i]} + \epsilon_{j[i]}
\] or more succinctly,

\[
Y_{i} = \alpha_{j[i]} + \beta X_{i} + \epsilon_{i}
\]

Another option for multilevel modelling is the \textbf{varying-slope
model} in which just the slope varies by group and the intercept is kept
constant.

\[
Y_{i} = \alpha + \beta_{j[i]} X_{i} + \epsilon_{i}
\]

Finally, the last mode in multilevl modelling is \textbf{varying-slope,
varying-intercept model} in which both the slope and intercept can vary
by group,

\[
Y_i = \alpha_{j[i]} + \beta_{j[i]}X_i + \epsilon_i
\] The varying slopes are interactions between the continuous predictor
X and the group indicators.

\subsection{7.2 Clustered data
structure}\label{clustered-data-structure}

With multilevel modeling, we go beyond the classical set up of a data
vector y and a matrix of predictors X. Each level of the model can have
its own matrix of predictors. Practically, the two-matrix level format
(where the data frame for each level is stored separately), has the
advantage of showing which information is available on an individual
level and on a group level clearly. It also does not repeat group level
information at the city level therefore removing redundancy from
computer memory.

\subsection{7.3 Experiment design}\label{experiment-design}

\subsubsection{7.3.1 Repeated measures}\label{repeated-measures}

Another kind of multilevel data structure involves repeated measurements
on units- thus measurements are clustered within units and predictors
can be available at the measurement or unit level.

\subsubsection{7.3.2 Time series cross sectional
data}\label{time-series-cross-sectional-data}

In settings where overall time trends are important, repeated
measurement data are sometimes called time series cross sectional.

\subsubsection{7.3.3 Non-nested strucutres}\label{non-nested-strucutres}

Non-nested data also arise when individuals are characterized by
overlapping categories of attributes. For example, consider a study of
earnings given occupation and state of residence. The data could include
1500 persons, 40 job categories and 50 states.

The log earnings model for a given individual would include demographic
predictors X, 40 indicators for job categories, and 50 state indicators.
We can write the model generalizing notation as,

\[
\begin{aligned}
y_i &= X_i \beta + \alpha_{j[i]} + \gamma_{k[i]} +\epsilon_i\\
\alpha_j &\sim N(U_j a, \sigma^2_\alpha)\\
\gamma_k &\sim N(V_kg, \sigma^2_\gamma)\\
\end{aligned}
\] where \(j[i]\) and \(k[i]\) represent the job category and state,
respectively, for person i. The model becomes multilevel with
regressions for the job and state coefficients. U is a matrix of
\(\alpha\) group level predictors, a is a vector of coefficients for the
\(\alpha\) level predictors and \(\sigma_a\) is the standard deviation
of the model errors for this level. Similarly, \(V_k\) is the matrix of
predictors for level \(\gamma\), g is the vector of coefficients for
\(\gamma\) level predictors and \(\sigma_\gamma\) is the standard
deviation of the mdoel errors at this level.

If groups with \(\gamma\) and \(\alpha\) don't overlap, the data is
non-nested.

The varying coefficients in a multilevel model are sometimes called
\textbf{random effects} , a term that refers to the randomness in the
probability model for the group-level coefficients. The term fixed
effect is used in contrast to random effects. \textbf{Fixed effects} are
usually defined as varying coefficients that are not themselves modeled.

\section{8. Multilevel modeling:
Basics}\label{multilevel-modeling-basics}

\emph{Notes from Gelman and Hill (2006), Chpater 12}

Multilevel modeling can be thought of in two ways

\begin{itemize}
\item{A generalization of linear regression, where intercepts, and possibly slopes are allowed to vary by group. For example, starting with a regression with one predictor, $y_i = \alpha + \beta x_i + \epsilon_i$, we can generalize to the varying-intercept model, $y_i = \alpha_{j[i]} + \beta x_{i} + \epsilon_i$ and a varying-intercept and varying-slope model with $y_i = \alpha_{j[i]} + \beta x_{j[i]} + \epsilon_i$}
\item{Equivalently, we can think of multilevel modeling as a regression that includes a categorical input variable representing group membership. From this perspective, the group index is a factor with J levels, corresponding to J predictors in the regression model.}
\end{itemize}

In either case, J-1 linear predictors are added to the model (in other
words, a constant term in the regression is replaced by J separate
intercept terms). The crucial multilevel modeling step is that these J
coefficients are then themselves given a model (most simple, a common
distribution for the J parameters \(\alpha_j\) or generally, a
regression model for the \(\alpha_j\)'2 given group level
predictors).The group level model is estiamted simultaneously with the
data level regression of y.

Multilevel modeling can be characterized as a compromise between two
extremes: \emph{complete pooling}, in which the group indicators are not
included in the model and \emph{no pooling} in which separate models are
fit within each group.

\textbf{Note on notation}

\begin{itemize}
\item{Units $i = 1, \cdots, n$, the smallest items of measurement e.g. individuals}
\item{Outcome measurements $y = (y_1, \cdots, y_n)$, response variable}
\item{Regression predictors are represented by a $n \times k$ matrix X, so that the vector of predicted values is $\hat{y} = X\beta$, where $\hat{y}$ and $\beta$ are column vectors of length n and k respectively. We include in X the constant term so that the first column of X is a 1's (to include an intercept term). Coefficients are labelled $\beta_0, \cdots, \beta_{k-1}$ or $\beta_1, \cdots, \beta_k$}
\end{itemize}

The two extremes in multilevel modeling are \emph{complete pooling}, in
which the group indicators are not included in the model and \emph{no
pooling}, in which separate models are fit within each group. A
compromise between these two extremes is partial-pooling. The form of a
partial pooling estimate is,

\[
\hat{\alpha}_j^{multilevel} \approx \frac{\frac{n_l}{\sigma^2_y}\bar{y}_j + \frac{1}{\sigma^2_\alpha}\bar{y}_{all}}{\frac{n_j}{\sigma^2_y} + \frac{1}{\sigma^2_\alpha}}
\]

The simplest multilevel model is a varying-intercept model written as

\[y_i \sim N(\alpha_{j[i]} + \beta x_i, \sigma_y^2)\]

In the complete pooling case, \(\alpha_j\)'s are given a ``hard''
constraint- they are all fixed at a common \(\alpha\). In the multilevel
model, a ``soft constraint'' is applied to the \(\alpha_j\)'s. They are
assigned a probability distribution,

\[\alpha_j \sim N(\mu_\alpha, \sigma^2_\alpha)\]

for \(j = 1, \cdots, J\), with their mean at \(\mu_\alpha\) and standard
deviation \(\sigma_\alpha\) estimated from the data.
\textcolor{red}{This would not be the case in Bayesian settings- the $\alpha$'s would come from a prior distribution, not constructed from the data unless it was an empirical bayesian setting right?}.

The distribution has the effect of pulling the estimates of \(\alpha_j\)
toward the mean level \(\mu_\alpha\), but not all the way, thus in each
county, a partial pooling compromise between the two estimates. In the
limit of \(\sigma_\alpha \rightarrow \infty\), the soft constraints do
nothing, and there is no pooling, as \(\sigma_\alpha \rightarrow 0\),
they pull the estimates all the way to zero, yielding the complete
pooling estimate. In other words, in complete pooling models,
\(\alpha_j\)s are given a hard constraint, they are all fixed at a
common \(\alpha\).

One way to interpret variation between levels is to consider the
variance ratio \(\sigma_\alpha^2/ \sigma^2_y\). Let this ratio
approximate to one-fifth. This means the standard deviation of y between
groups is the same as the standard deviation of the average 5
measurements within a group. The relative values of individual- and
group-level variances are also sometimes expressed using the interclass
correlation \(\sigma^2_\alpha / (\sigma^2_\alpha + \sigma^2_y)\), which
ranges from 0 if the grouping conveys no information to 1 if all group
members are identical. Further, for a group sample size less than 5,
there is more information in the group-level model than in the group's
data; for a group with more than 5 observations, the within-group
measurements are more informative. In summary, the multilevel regression
line in a group is closer to complete-pooling estimate with the sample
size is less than 5, and closer to no-pooling estimate with sample size
exceeds 5.

Note: The term ``fixed effects'' is used for regression coefficients
that do not vary by group or for group-level coefficients or group
averages (such as the average intercept \(\mu_\alpha\)).

\textcolor{red}{ORTHODONTICS DATA EXAMPLE}

\section{11. Dependent Data}\label{dependent-data}

\emph{Notes from Chapter 8, Bayesian and Frequentist Regression Methods}

Dependent data occur in common situations such as when sampling is over
time, space or within groups/families. Generally, this considers
regression modeling situations in which there are a set of units
(``clusters'') upon which multiple measurements have been collected.
When data are available over time for a group of units, we have
\emph{longitudinal} data, and each unit (i.e.~individual) forms a
cluster. In the context of mixed effect models, ``marginal'' refers to
an averaging of data across the group. It is on the other extreme of
having a distinct curve for each group or unit.

Refering to the same orthodontics data example, with the marginal
approach, our model is,

\[\mathbb{E}[Y_{ij}] = \beta_0^M + \beta_1^Mt_j\] where \(\beta_0^M\)
and \(\beta_1^M\) represent marginal intercept and slope parameters.
Then,

\[e^M_{ij} = Y_{ij} - \beta_0^M - \beta_1^Mt_j\] where i indexes the
child and j indexes the time steps. In this data, \(i = 1, \cdots, 11\)
and \(j = 1, \cdots, 4\).

Due to the dependence of observations on the same girl, we would not
expect the marginal residuals to be independent. To take into account
the depedence, data are modeled so that the off-diagonals in the
covariance is non-zero and is given some amount of correlation
\(\rho_{ij}\). Diagonals are standard deviations.

\[\sigma_j = \sqrt{\text{var}(e_{ij}^M)}\] This is the standard
deviation of the dental length at time \(t_j\) and,

\[\rho_{jk} = \frac{\text{cov}(e_{ij}^M, e_{ik}^M)}{\sqrt{\text{var}(e_{ij}^M)\text{var}(e_{ik}^M)}}\]
is the correlation between residual measurements taken at time \(t_j\)
and \(t_k\) on the same individual. This is accounting for the
dependence between observations on the same individual as we would
expect observations arising from the same individual to be more similar
than observations from different individuals.

The basis of using a mixed effects model for longitudinal data is to
assume that a unit (or individual) specific set of random effects are
assumed to arise from a population.

In different contexts, random effects may have a direct interpretation
as arising from a population of effects or may simply be viewed as a
convenient modeling tool, in situations in which there is no
hypothetical population of effects to appeal to.

\subsection{Writing models in R}\label{writing-models-in-r}

A complete pooling model would be written as

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{completePool <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x)}
\end{Highlighting}
\end{Shaded}

A no-pooling model would be written as

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{noPool <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x }\OperatorTok{+}\StringTok{ }\KeywordTok{factor}\NormalTok{(group))}
\end{Highlighting}
\end{Shaded}

A partial pooling model would be written as

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{partialPool <-}\StringTok{ }\KeywordTok{lmer}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{|}\StringTok{ }\NormalTok{group))}

\NormalTok{partialPool2 <-}\StringTok{ }\KeywordTok{lmer}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{|}\StringTok{ }\NormalTok{group)) }\CommentTok{#no predictors, just varying intercepts. }
\end{Highlighting}
\end{Shaded}

\subsection{5 ways to write the same
model}\label{ways-to-write-the-same-model}

\textit{Allowing regression coefficients to vary across groups}
Classical regression model for all the data,

\[
y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots + \epsilon_i
\]

Generalizing the model to allow coefficients \(\beta\) to vary across
groups,

\[
y_i = \beta_{0 j[i]} + \beta_{1 j[i]} X_{i1} + \beta_{2j[i]}X_{i2} + \cdots + \epsilon_i
\]

This involves assigning a multivariate distribution to the vector of
\(\beta\)'s within each group.

Varying intercept models where the only coefficient that varies across
groups is the constant term \(\beta_0\),

\[
y_i = \beta_{0j[i]} + \beta_1X_{i1} + \beta_2X_{i2} + \epsilon_i
\] where, \[\beta_{0j} \sim N(\mu, \sigma^2)\] where
\[\beta_{0j} = \mu + \eta_j\] with \[\eta_j \sim N(0, \sigma^2)\]

\textit{Combining separate local regressions} An alternative way to
write the multilevel model is as a linking of local regressions in each
group. Within each group j, a regression is performed on the local
predictors, with a constant term \(\beta_0\) that is indexed by group,
In group j, \[
y_i \sim N(\beta_{0j} + \beta x_i, \sigma^2_y)
\] Then group level predictors are included as group level predictors
for the second level of the model,

\[
\beta_{0j} \sim N(\gamma_0 + \gamma_1 \mu_j, \sigma^2)
\] This multilevel model combines the J local regression models in two
ways: first the local regression coefficients \(\beta\) are the same in
all J models, second the different intercepts \(\alpha_j\) are connected
through the group level model.

\textit{Modeling the coefficients of a large regression model} The
identical model can be writted as a single regression, in which the
local and group level predictors are combined into a single matrix X,

\[y_i \sim N(X_i \beta, \sigma^2_y)\]

where X includes vectors corresponding to,

A constant term, \(X_{(0)}\), \(X_{(1)}\), \(X_{(2)}\), J (not J-1)
group indicators, \(X_{(3)}, \cdots, X_{(J + 2)}\)

The J county indicators (which in this case are
\(\beta_3, \cdots, \beta_{J + 2}\)) follow a normal distribution:

\[\beta_j \sim N(0, \sigma^2)\]

\textcolor{red}{In this case the $\beta_j$ distribution is centered at 0 rather than at an estiamted $\mu_\beta$ because any such $\mu_\beta$ would be statistically indistinguishable from the constant term in the regression - ? }

\textit{Moving the constant term around}

The multilevel model can be writted another way by moving the constant
term,

\[
\begin{aligned}
y_i &= N(X_i\beta, \sigma^2_y), \qquad \text{for } i = 1, \cdots, n\\
\beta_j &\sim N(\mu, \sigma^2), \qquad \text{for } j = 3, \cdots, J + 2\\
\end{aligned}
\]

In this version, the constant term from X has been removed and replaced
by the equivalent term \(\mu\) in the group level model. The
coefficients \(\beta_3, \cdots, \beta_{J + 2}\) for the group indicators
are now centered around \(\mu\) and are equivalent to \(\beta_{0[j]}\)
from earlier models.

\textit{Regression with multiple error terms} Re-express the model bu
treating the group level indicator coefficients as error terms rather
than regression coefficients,

\[
\begin{aligned}
y_i &\sim N(X_i \beta + \eta_{j[i]}, \sigma^2_y),\\
\eta_j &\sim N(0, \sigma^2)
\end{aligned}
\]

\textit{Large regression with correlated errors}

Finally, we can express a multilevel model with correlated errors,

\[
\begin{aligned}
y_i &= X_i\beta + \epsilon^{all}_i\\
\epsilon^{all} &\sim N(0, \Sigma)\\
\end{aligned}
\]

where X now has three predictors and the errors have an \(n \times n\)
covariance matrix. \(\Sigma\) is paramterized as,

For unit i:
\(\Sigma_{ii} = \mathbb{V}[\epsilon^{all}_i] = \sigma^2_y + \sigma^2\).
For unit i,k in same group j,
\(\Sigma_{ik} = \mathbb{C}ov[\epsilon^{all}_i, \epsilon^{all}_k] = \sigma^2\)
For unit i,k in different groups,
\(\Sigma_{ik} = \mathbb{C}ov[\epsilon^{all}_i, \epsilon^{all}_k] = 0\).

\textcolor{red}{What about for multivariate cases?}

When is multilevel modeling most effective? Multilevel modeling is most
important when it is close to \textbf{complete pooling}, atleast for
some groups. Estimates are more pooled when the group standard deviation
\(\sigma_\alpha\) is small, i.e., when the groups are similar to each
other. In contrast, when \(\sigma_\alpha\) is large, so that the groups
vary greatly, multlevel modeling is not much better than simple
no-pooling estimation.

\textcolor{red}{How do you test statistical significance of varying intercepts? Chp12 pg271}

\section{9. Multilevel Linear Models: Varying slopes, non-nested models,
and other
complexities}\label{multilevel-linear-models-varying-slopes-non-nested-models-and-other-complexities}

Beyond varying intercepts are models than allow varying intercepts AND
slopes.

\[
\begin{aligned}
y_i &\sim N (\alpha_{j[i]} + \beta_{j[i]}x_i, \sigma^2_y)\\
\begin{pmatrix}
\alpha_j\\
\beta_j\\
\end{pmatrix} &\sim N \left(\begin{pmatrix}\mu_\alpha\\\mu_\beta\\ 
\end{pmatrix}, 
\begin{pmatrix}
\sigma^2_\alpha & \rho \sigma_\alpha \sigma_\beta\\
\rho \sigma_\alpha \sigma_\beta & \sigma^2_\beta
\end{pmatrix} 
\right)\\
\end{aligned}
\] The covariance structure describes the variation in \(\alpha_j\)'s
and \(\beta_j\)'s and also a between group correlation parameter
\(\rho\).

In R code this is written as,

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(lme4)}
\KeywordTok{lmer}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{x }\OperatorTok{|}\StringTok{ }\NormalTok{group))}
\end{Highlighting}
\end{Shaded}

In this model, the unexplained within-county variation has an estimated
standard deviation of \(\hat{\sigma}^2_y\), the estimated standard
deviation of the group intercepts is \(\hat{\sigma}_\alpha\), the
estimated standard deviation of the group slopes is
\(\hat{\sigma}_\beta\). The estimated correlation between slopes and
intercepts is \(\rho\).

Extending this model by including a group-level predictor,

\[
\begin{aligned}
y_i &\sim N (\alpha_{j[i]} + \beta_{j[i]}x_i, \sigma^2_y)\\
\begin{pmatrix}
\alpha_j\\
\beta_j\\
\end{pmatrix} &\sim N \left(\begin{pmatrix}\gamma^\alpha_0 + \gamma_1^\alpha \mu_j\\\ \gamma_0^\beta + \gamma_1^\beta \mu_j\\ 
\end{pmatrix}, 
\begin{pmatrix}
\sigma^2_\alpha & \rho \sigma_\alpha \sigma_\beta\\
\rho \sigma_\alpha \sigma_\beta & \sigma^2_\beta
\end{pmatrix} 
\right)\\
\end{aligned}
\]

\textcolor{red}{Why can we think of models with varying slope and intercepts as interactions between group indicators and an individual-level predictor?}
Because when we estimate models with varying slopes and varying
intercepts, there will be a group level indicator for slope and
intercept multiplied by the individual level predictor (x).

In terms of R code this is,

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lmer}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x }\OperatorTok{+}\StringTok{ }\NormalTok{U.full }\OperatorTok{+}\StringTok{ }\NormalTok{x}\OperatorTok{:}\NormalTok{U.full }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{x}\OperatorTok{|}\NormalTok{group))}
\end{Highlighting}
\end{Shaded}

The model above can be broken down into,

\[
\begin{aligned}
y_i &= \alpha_{j[i]} + \beta_{j[i]}x_i + \epsilon_i\\
\alpha_j &= \gamma^\alpha_0 + \gamma_1^\alpha \mu_j + \eta^\alpha_j\\
\beta_j &= \gamma^\beta_0 + \gamma^\beta_1 \mu_j + \eta^\beta_j\\
\end{aligned}
\] We can re-express this as a single model by substituting formulas for
\(\alpha_j\) and \(\beta_j\),

\[y_i = [\gamma^\alpha_0 + \gamma_1^\alpha \mu_j + \eta^\alpha_j] + [\gamma^\beta_0 + \gamma^\beta_1 \mu_j + \eta^\beta_j]x_i + \epsilon_i\]
If we define a new individual-level predictor \(\nu_i = \mu_{j[i]}\), we
can re-express the above equation as,

\[y_i = a + b \nu_i + c_{j[i]} + dx_i + e\nu_ix_i + f_{j[i]}x_i + \epsilon_i\]

This can be thought of is a number of ways,

\textbackslash{}begin\{itemize\} \item\{A varying intercept, varying
slope model with four individual-level predictors (the constant term
\(v_i, x_i\) and the interaction \(v_i x_i\)) and the varying intercepts
and slopes that are centered at 0\} \item\{A regression model with 4 +
2J predictors: the constant term \(v_i, x_i, (v_ix_i)\) and two group
level predictors \(c_{j[i]}, f_{j[i]\)\} \item\{A regression model with
four predictors and three error terms\} \textbackslash{}end\{itemize\}

\subsection{9.1 Understanding correlations in varying slope-intercept
model}\label{understanding-correlations-in-varying-slope-intercept-model}

The varying slopes can be interpreted as interactions between an
individual predictor and group indicators. As with classical regression
models with interactions, the intercepts can often be more clearly
interpreted if the continuous predictor is appropriately centered. When
there is a very high correlation, it is useful to subtract the average
value fo the continuous x before including it in the regression, to
scale it.

\section{10. Detecting Influential Data in Mixed Effect
Models}\label{detecting-influential-data-in-mixed-effect-models}

\emph{(from Nieuwenhuis et al., 2012)}

This section of notes is based on lectures on diagnostic tools for
influential data for mixed modela. These measures include DFBETAS,
Cook's distance and percentile change and a test for changing levels of
significance. It is commonly accepted that tests for influential data
should be performed on regression models, especially when estimates are
based on a small number of cases. However, existing procedures do not
account for the nesting structure of the data. As a result, these
existing procedures fail to detect that higher-level cases may be
influential on estimates of variables measured at specifically that
level.

Testing for influential cases in mixed effect regression models is
important, because influential data negatively influence the statistical
fit and generalizability of the model.

All cases used to estimate a regression model exert some level of
influence on the regression parameters. However, if a single case has
extremely high or low scores on the dependent variable relative to its
expected value, this case may overly influence the regression parameters
by pulling the estimated regression line towards itself. The simple
inclusion or exclusion of such a single case may then lead to
substantially different regression estimates. This runs against
distributional assumptions associated with regression models, as a
result limits the validity and generalizability of regression models in
which influential cases are present.

The analysis of residuals cannot be used for the detection of
influential cases. Cases with high residuals (defined as the difference
between the observed and predicted scores on the dependent variable) or
with high standardized residuals are indicated as outliers. On the
contrary, a strongly influential case dominates the regression model in
such a way that the estimated regression line lies closely to this case.
Although influental cases thus have extreme values on one or more of the
variables, they can be \emph{onliers} rather than \emph{outliers}. To
account for this, \emph{standardized deleted residuals} are defined as
the difference between the observed score of a case on the dependent
variable and the predictor score from the regression model that is based
on data from which the case was removed.

Just as influential cases are not necessarily outliers, outliers are not
necessarily influential cases. The reason for this is the influence a
case exerts on the regression slope is not only determined by how well
its (observed) score is fitted by the specified regression model, but
also by its score(s) on the independent variable(s).

The degrees to which the scores of a case on the independent variable(s)
are extreme is indicated by the leverage of this case. A higher leverage
means more extreme scores on the independent variables and a greater
potential of overly influencing the regression outcomes.

The basic rationale behind measuring influenctial cases is based on the
principle that when single cases are iteratively omitted from the data,
models based on these data should not produce substantially different
estimates. If the model parameters change substantially after a single
case is excluded, this case may be regarded as too influential.

How much change in the model parameters is acceptable? DFBETAS is a
standardized measure of the absolute difference between the estimate
with a particular case included and the estimate without that particular
case. Cook's distance provides an overall measurement of the change in
all parameter estiamtes, or a selection thereof. In addtion, measure of
percentile change and a test for changing levels of signficance of the
fixed parameters.

This discussion can be extended from how single cases can overly
influence the point estimates (or betas) of a regression model to
biasing confidence intervals of these estiamtes. Cases with high
leverage can be influential because of their extreme values on the
independent variables. Cases with a high leverage but a low deleted
residual compress standard errors, while cases with low leverage and a
high deleted residual inflate standard errors.

\textcolor{red}{To apply the logic of detecting influential data to generalized mixed effects models, one has to measure the influence of a particular higher level group
on the estimates of a predictor measured at that level.
The straightforward way is to delete all observations
from the data that are nested within a single higher
level group, then re-estimate the regression model,
and finally evaluate the change in the estimated regression parameters. This procedure is then repeated
for each higher-level group separately.}

\subsection{10.1 DFBETAS}\label{dfbetas}

DFBETAS is a standardized measure that indicates the level of influence
oobservations have on a single aprameter. Regarding mixed models, this
relates to the influence a higher level unit has on the parameter
estimate. DFBETAS is calculated as the difference in the magnitude of
the parameter estimate between the model including and the model
excluding the higher level case.

\[DFBETAS_{ij} = \frac{\hat{\gamma_i} - \hat{\gamma}_{i(-j)}}{se(\hat{\gamma}_{i(-j)})}\]

where i refers to the parameter estiamte, and j the higher level group
so that \(\hat{\gamma}_i\) represnts the original estiamte of parameter
i and \(\hat{\gamma}_{i(-j)}\) represents the estimate of parameter i,
after the higher level group j has been excluded from the data.

Cut off value for DFBETAS is \(2/ \sqrt{n}\)

\subsection{10.2 Cook's distance}\label{cooks-distance}

Since DFBETAS provides a value for each parameter and for each
higher-level unit that is evaluated, this often results in quite a large
number of values to evaluate. An alternative is Cook's distnace-
provides a summary measure for the influence a higher level unit exerts
on all parameters simultaneously.

\[C_j^{0F} = \frac{1}{r +1}\left(\hat{\gamma} - \hat{\gamma}_{(-j)}\right) \hat{\Sigma}_F^{-1}\left(\hat{\gamma} - \hat{\gamma}_{(-j)}\right)\]

where \(\hat{\gamma}\) represents the vector of the original parameter
estimates, \(\hat{\gamma}_{(-j)}\) the parameter estimates of the model
excluding the higher level unit j, and \(\hat{\Sigma}_F\) represents the
covariance matrix. r is the number of parameters that are evaluated,
excluding the intercept vector.

The cut off value for Cook's distance is \(4/n\) where n refers to
number of groups in grouping factor under evaluation.

\textbf{Note:} If Cook's distance is calculated based on a single
parameter, the Cook's distance equals the squared value of the DFBETAS
for that parameter.

\subsection{10.3 Percentile Change}\label{percentile-change}

For substantive interpretation of the model outcomes, the relative
degree to which a parameter estimate changes may provide more meaningful
information. For each higher level group, the percentage of change is
calculated as the absolute difference between the parameter estimate
both including and excluding the higher-level unit, divided by the
parameter estimate of the complete model and multiplied by 100\%. A
percentage of change is returned for each parameter separately, for each
of the higher-level units under investigation.

\[\left(\hat{\gamma} - \hat{\gamma}_{(-j)}\right)\frac{1}{\hat{\gamma}} \times 100
\% \]

\section{12. Mixed-Effects Models: Bayesian
Inference}\label{mixed-effects-models-bayesian-inference}

\section{13. Case studies}\label{case-studies}

\subsection{13.1 Reed frogs}\label{reed-frogs}

\textit{From Rethinking Statistics}

Think of each row as a tank, an experimental environment that contains
tadpoles. There are lots of things peculiar to each tank that go
unmeasured, and these unmeasured factors create variation in survival
across tanks, even when all predictor variables have the same value. If
these ``clusters'' are ignored, we might be ignoring important variation
in baseline survival.

To accommodate variation produced by differen tanks, we can create a
varying effects model. A varying INTERCEPTS model is the simplest kind
of varying effects model. A random intercept model is no different than
a categorical variable for tank model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reedfrogs <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"~/Documents/R/STAT610/reedfrogsCLEAN.csv"}\NormalTok{, }\DataTypeTok{stringsAsFactors =}\NormalTok{ T)}
\KeywordTok{colnames}\NormalTok{(reedfrogs)[}\DecValTok{1}\NormalTok{] <-}\StringTok{ "density"}

\KeywordTok{require}\NormalTok{(brms)}
\KeywordTok{require}\NormalTok{(tidyverse)}
\KeywordTok{require}\NormalTok{(dplyr)}

\NormalTok{reedfrogs <-}\StringTok{ }
\StringTok{  }\NormalTok{reedfrogs }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{tank =} \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(reedfrogs))}

\KeywordTok{head}\NormalTok{(reedfrogs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   density pred  size surv propsurv tank
## 1      10   no   big    9      0.9    1
## 2      10   no   big   10      1.0    2
## 3      10   no   big    7      0.7    3
## 4      10   no   big   10      1.0    4
## 5      10   no small    9      0.9    5
## 6      10   no small    9      0.9    6
\end{verbatim}

\[
\begin{aligned}
\text{surv}_i &\sim \text{Binomial}(n_i, p_i)\\
\text{logit}(p_i) &= \alpha_{\text{tank}_i}\\
\alpha_{\text{tank}} &\sim \text{Normal}(0,5)
\end{aligned}
\]

Notes on the model- here, we are using logistic regression to predict
survival. Probability of survival is modelled as a random intercept for
each record in the given dataframe.

Fitting a model

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b1 <-}\StringTok{ }\KeywordTok{brm}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ reedfrogs, }\DataTypeTok{family =}\NormalTok{ binomial,}
\NormalTok{          surv}\OperatorTok{|}\KeywordTok{trials}\NormalTok{(density) }\OperatorTok{~}\StringTok{ }\DecValTok{0} \OperatorTok{+}\StringTok{ }\KeywordTok{factor}\NormalTok{(tank), }
          \KeywordTok{prior}\NormalTok{(}\KeywordTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{5}\NormalTok{), }\DataTypeTok{class =}\NormalTok{ b),}
          \DataTypeTok{iter =} \DecValTok{2000}\NormalTok{, }\DataTypeTok{warmup =} \DecValTok{500}\NormalTok{, }\DataTypeTok{chains =} \DecValTok{4}\NormalTok{,}
          \DataTypeTok{cores =}\NormalTok{ parallel}\OperatorTok{::}\KeywordTok{detectCores}\NormalTok{(), }
          \DataTypeTok{seed =} \DecValTok{12}\NormalTok{)}
\CommentTok{#this will give us 48 intercepts- one for each tank}
\end{Highlighting}
\end{Shaded}

An alternative model is

\[
\begin{aligned}
\text{surv}_i &\sim \text{Binomial}(n_i, p_i)\\
\text{logit}(p_i) &= \alpha_{\text{tank}_i}\\
\alpha_{\text{tank}}&\sim \text{Normal}(\alpha, \sigma)\\
\alpha &\sim \text{Normal}(0,1)\\
\sigma &\sim \text{HalfCauchy}(0,1)\\
\end{aligned}
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b1.}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{brm}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ reedfrogs, }\DataTypeTok{family =}\NormalTok{ binomial,}
\NormalTok{            surv}\OperatorTok{|}\KeywordTok{trials}\NormalTok{(density) }\OperatorTok{~}\StringTok{ }\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{|}\NormalTok{tank), }
            \DataTypeTok{prior =} \KeywordTok{c}\NormalTok{(}\KeywordTok{prior}\NormalTok{(}\KeywordTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{class =}\NormalTok{ Intercept),}
                      \KeywordTok{prior}\NormalTok{(}\KeywordTok{cauchy}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{class =}\NormalTok{ sd)),}
            \DataTypeTok{iter =} \DecValTok{4000}\NormalTok{, }\DataTypeTok{warmup =} \DecValTok{1000}\NormalTok{, }\DataTypeTok{chains =} \DecValTok{4}\NormalTok{, }
            \DataTypeTok{cores =}\NormalTok{ parallel}\OperatorTok{::}\KeywordTok{detectCores}\NormalTok{(), }
            \DataTypeTok{seed =} \DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this case, 1 + (1\textbar{}tank) indicates only tthe intercept, 1,
varies by tank. The extent to which parameters vary is controlled by the
prior (cauchy prior).

Let's do model selections

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b1  <-}\StringTok{ }\KeywordTok{add_criterion}\NormalTok{(b1, }\StringTok{"waic"}\NormalTok{)}
\NormalTok{b1.}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{add_criterion}\NormalTok{(b1.}\DecValTok{2}\NormalTok{, }\StringTok{"waic"}\NormalTok{)}

\NormalTok{w <-}\StringTok{ }\KeywordTok{loo_compare}\NormalTok{(b1, b1.}\DecValTok{2}\NormalTok{, }\DataTypeTok{criterion =} \StringTok{"waic"}\NormalTok{)}

\KeywordTok{print}\NormalTok{(w, }\DataTypeTok{simplify =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic  
## b1.2    0.0       0.0  -100.0       3.6         21.1    0.9     200.1
## b1     -0.2       2.2  -100.2       4.6         22.4    0.6     200.5
##      se_waic
## b1.2    7.2 
## b1      9.2
\end{verbatim}

pWAIC tells us something about how many ``effective parameters'' there
are in the model. In this case, b1 has \(\sim\) 24 and b1.2 has \(\sim\)
21. There are fewer effective parameters than actual parameters because
the other parameters are shrunk towards mean \(\alpha\). The model with
the fewer number of effective parameters indicates more shrinkage.

Compare with LOO

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b1 <-}\StringTok{ }\KeywordTok{add_criterion}\NormalTok{(b1, }\StringTok{"loo"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Found 44 observations with a pareto_k > 0.7 in model 'b1'. With
## this many problematic observations, it may be more appropriate to use
## 'kfold' with argument 'K = 10' to perform 10-fold cross-validation rather
## than LOO.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b1.}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{add_criterion}\NormalTok{(b1.}\DecValTok{2}\NormalTok{, }\StringTok{"loo"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Found 40 observations with a pareto_k > 0.7 in model 'b1.2'.
## With this many problematic observations, it may be more appropriate to use
## 'kfold' with argument 'K = 10' to perform 10-fold cross-validation rather
## than LOO.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{w2 <-}\StringTok{ }\KeywordTok{loo_compare}\NormalTok{(b1, b1.}\DecValTok{2}\NormalTok{, }\DataTypeTok{criterion =} \StringTok{"loo"}\NormalTok{)}
\KeywordTok{print}\NormalTok{(w2, }\DataTypeTok{simplfy =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      elpd_diff se_diff
## b1.2  0.0       0.0   
## b1   -0.7       3.1
\end{verbatim}

Let's visualize shrinkage

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{post <-}\StringTok{ }\KeywordTok{posterior_samples}\NormalTok{(b1.}\DecValTok{2}\NormalTok{, }\DataTypeTok{add_chain =}\NormalTok{ T)}

\NormalTok{post_mdn <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(b1.}\DecValTok{2}\NormalTok{, }\DataTypeTok{robust =}\NormalTok{ T)}\OperatorTok{$}\NormalTok{tank[, ,] }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{as_tibble}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{bind_cols}\NormalTok{(reedfrogs)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{post_mdn =} \KeywordTok{inv_logit_scaled}\NormalTok{(Estimate))}

\KeywordTok{glimpse}\NormalTok{(post_mdn)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Observations: 48
## Variables: 11
## $ Estimate  <dbl> 2.0680144, 2.9313918, 0.9769485, 2.9240152, 2.065459...
## $ Est.Error <dbl> 0.8472571, 1.0630858, 0.6613001, 1.0436165, 0.829300...
## $ Q2.5      <dbl> 0.6008842, 1.1628800, -0.2400727, 1.1968145, 0.64441...
## $ Q97.5     <dbl> 4.009865, 5.430653, 2.378882, 5.361220, 3.947779, 4....
## $ density   <int> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, ...
## $ pred      <fct> no, no, no, no, no, no, no, no, pred, pred, pred, pr...
## $ size      <fct> big, big, big, big, small, small, small, small, big,...
## $ surv      <int> 9, 10, 7, 10, 9, 9, 10, 9, 4, 9, 7, 6, 7, 5, 9, 9, 2...
## $ propsurv  <dbl> 0.90, 1.00, 0.70, 1.00, 0.90, 0.90, 1.00, 0.90, 0.40...
## $ tank      <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1...
## $ post_mdn  <dbl> 0.8877553, 0.9493766, 0.7265023, 0.9490209, 0.887500...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(ggthemes)}
\NormalTok{post_mdn }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ tank)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept =} \KeywordTok{inv_logit_scaled}\NormalTok{(}\KeywordTok{median}\NormalTok{(post}\OperatorTok{$}\NormalTok{b_Intercept)), }\DataTypeTok{linetype =} \DecValTok{2}\NormalTok{, }\DataTypeTok{size =} \DecValTok{1}\OperatorTok{/}\DecValTok{4}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_vline}\NormalTok{(}\DataTypeTok{xintercept =} \KeywordTok{c}\NormalTok{(}\FloatTok{16.5}\NormalTok{, }\FloatTok{32.5}\NormalTok{), }\DataTypeTok{size =} \DecValTok{1}\OperatorTok{/}\DecValTok{4}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ propsurv), }\DataTypeTok{color =} \StringTok{"orange2"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ post_mdn), }\DataTypeTok{shape =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{coord_cartesian}\NormalTok{(}\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_x_continuous}\NormalTok{(}\DataTypeTok{breaks =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{32}\NormalTok{, }\DecValTok{48}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Multilevel shrinkage!"}\NormalTok{, }
       \DataTypeTok{subtitle =} \StringTok{"The empirical proportions are in orange while the model-}\CharTok{\textbackslash{}n}\StringTok{ implied proportions are the black circles. The dashed line is}\CharTok{\textbackslash{}n}\StringTok{the model-implied average survival proportion"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{, }\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{16} \OperatorTok{+}\StringTok{ }\DecValTok{8}\NormalTok{, }\DecValTok{32} \OperatorTok{+}\StringTok{ }\DecValTok{8}\NormalTok{), }\DataTypeTok{y =} \DecValTok{0}\NormalTok{, }
           \DataTypeTok{label =} \KeywordTok{c}\NormalTok{(}\StringTok{"small tanks"}\NormalTok{, }\StringTok{"medium tanks"}\NormalTok{, }\StringTok{"large tanks"}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{theme_fivethirtyeight}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{panel.grid =} \KeywordTok{element_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{Hierarchical_notes_files/figure-latex/unnamed-chunk-13-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plotDat <-}\StringTok{ }\NormalTok{post }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{sample_n}\NormalTok{(}\DecValTok{100}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{expand}\NormalTok{(}\KeywordTok{nesting}\NormalTok{(iter, b_Intercept, sd_tank__Intercept), }
         \DataTypeTok{x =} \KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \OperatorTok{-}\DecValTok{4}\NormalTok{, }\DataTypeTok{to =} \DecValTok{5}\NormalTok{, }\DataTypeTok{length.out =} \DecValTok{100}\NormalTok{))}

\NormalTok{plotDat <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(plotDat)}
  
  \KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ plotDat, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{group =}\NormalTok{ iter)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =} \KeywordTok{dnorm}\NormalTok{(plotDat[,}\DecValTok{4}\NormalTok{], plotDat[,}\DecValTok{2}\NormalTok{], plotDat[, }\DecValTok{3}\NormalTok{])), }
            \DataTypeTok{alpha =}\NormalTok{ .}\DecValTok{2}\NormalTok{, }\DataTypeTok{color =} \StringTok{"orange2"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Population survival distribution"}\NormalTok{, }
       \DataTypeTok{subtitle =} \StringTok{"The Gaussian are on the log-odds scale."}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{breaks =} \OtherTok{NULL}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{coord_cartesian}\NormalTok{(}\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{theme_fivethirtyeight}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{plot.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{13}\NormalTok{), }
        \DataTypeTok{plot.subtitle =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Hierarchical_notes_files/figure-latex/unnamed-chunk-14-1.pdf}
The uncertainty in this plot is obvious both in the location of
\(\alpha\) and in the scale \(\sigma\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b1.2e <-}\StringTok{ }\KeywordTok{update}\NormalTok{(b1.}\DecValTok{2}\NormalTok{, }
                \DataTypeTok{prior =} \KeywordTok{c}\NormalTok{(}\KeywordTok{prior}\NormalTok{(}\KeywordTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{class =}\NormalTok{ Intercept), }
                          \KeywordTok{prior}\NormalTok{(}\KeywordTok{exponential}\NormalTok{(}\DecValTok{1}\NormalTok{), }\DataTypeTok{class =}\NormalTok{ sd)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## SAMPLING FOR MODEL '91df5d26f10fabfca4973a274312d58d' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 3.9e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.39 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 4000 [  0%]  (Warmup)
## Chain 1: Iteration:  400 / 4000 [ 10%]  (Warmup)
## Chain 1: Iteration:  800 / 4000 [ 20%]  (Warmup)
## Chain 1: Iteration: 1001 / 4000 [ 25%]  (Sampling)
## Chain 1: Iteration: 1400 / 4000 [ 35%]  (Sampling)
## Chain 1: Iteration: 1800 / 4000 [ 45%]  (Sampling)
## Chain 1: Iteration: 2200 / 4000 [ 55%]  (Sampling)
## Chain 1: Iteration: 2600 / 4000 [ 65%]  (Sampling)
## Chain 1: Iteration: 3000 / 4000 [ 75%]  (Sampling)
## Chain 1: Iteration: 3400 / 4000 [ 85%]  (Sampling)
## Chain 1: Iteration: 3800 / 4000 [ 95%]  (Sampling)
## Chain 1: Iteration: 4000 / 4000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.335981 seconds (Warm-up)
## Chain 1:                0.832571 seconds (Sampling)
## Chain 1:                1.16855 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL '91df5d26f10fabfca4973a274312d58d' NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 2.1e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 4000 [  0%]  (Warmup)
## Chain 2: Iteration:  400 / 4000 [ 10%]  (Warmup)
## Chain 2: Iteration:  800 / 4000 [ 20%]  (Warmup)
## Chain 2: Iteration: 1001 / 4000 [ 25%]  (Sampling)
## Chain 2: Iteration: 1400 / 4000 [ 35%]  (Sampling)
## Chain 2: Iteration: 1800 / 4000 [ 45%]  (Sampling)
## Chain 2: Iteration: 2200 / 4000 [ 55%]  (Sampling)
## Chain 2: Iteration: 2600 / 4000 [ 65%]  (Sampling)
## Chain 2: Iteration: 3000 / 4000 [ 75%]  (Sampling)
## Chain 2: Iteration: 3400 / 4000 [ 85%]  (Sampling)
## Chain 2: Iteration: 3800 / 4000 [ 95%]  (Sampling)
## Chain 2: Iteration: 4000 / 4000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.350799 seconds (Warm-up)
## Chain 2:                0.808433 seconds (Sampling)
## Chain 2:                1.15923 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL '91df5d26f10fabfca4973a274312d58d' NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 3.3e-05 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.33 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 4000 [  0%]  (Warmup)
## Chain 3: Iteration:  400 / 4000 [ 10%]  (Warmup)
## Chain 3: Iteration:  800 / 4000 [ 20%]  (Warmup)
## Chain 3: Iteration: 1001 / 4000 [ 25%]  (Sampling)
## Chain 3: Iteration: 1400 / 4000 [ 35%]  (Sampling)
## Chain 3: Iteration: 1800 / 4000 [ 45%]  (Sampling)
## Chain 3: Iteration: 2200 / 4000 [ 55%]  (Sampling)
## Chain 3: Iteration: 2600 / 4000 [ 65%]  (Sampling)
## Chain 3: Iteration: 3000 / 4000 [ 75%]  (Sampling)
## Chain 3: Iteration: 3400 / 4000 [ 85%]  (Sampling)
## Chain 3: Iteration: 3800 / 4000 [ 95%]  (Sampling)
## Chain 3: Iteration: 4000 / 4000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.34147 seconds (Warm-up)
## Chain 3:                0.810343 seconds (Sampling)
## Chain 3:                1.15181 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL '91df5d26f10fabfca4973a274312d58d' NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 2.1e-05 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 4000 [  0%]  (Warmup)
## Chain 4: Iteration:  400 / 4000 [ 10%]  (Warmup)
## Chain 4: Iteration:  800 / 4000 [ 20%]  (Warmup)
## Chain 4: Iteration: 1001 / 4000 [ 25%]  (Sampling)
## Chain 4: Iteration: 1400 / 4000 [ 35%]  (Sampling)
## Chain 4: Iteration: 1800 / 4000 [ 45%]  (Sampling)
## Chain 4: Iteration: 2200 / 4000 [ 55%]  (Sampling)
## Chain 4: Iteration: 2600 / 4000 [ 65%]  (Sampling)
## Chain 4: Iteration: 3000 / 4000 [ 75%]  (Sampling)
## Chain 4: Iteration: 3400 / 4000 [ 85%]  (Sampling)
## Chain 4: Iteration: 3800 / 4000 [ 95%]  (Sampling)
## Chain 4: Iteration: 4000 / 4000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.327268 seconds (Warm-up)
## Chain 4:                0.828656 seconds (Sampling)
## Chain 4:                1.15592 seconds (Total)
## Chain 4:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(b1.2e)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Family: binomial 
##   Links: mu = logit 
## Formula: surv | trials(density) ~ 1 + (1 | tank) 
##    Data: reedfrogs (Number of observations: 48) 
## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1;
##          total post-warmup samples = 12000
## 
## Group-Level Effects: 
## ~tank (Number of levels: 48) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     1.57      0.21     1.21     2.04 1.00     3234     5009
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.35      0.25     0.86     1.84 1.00     2666     4032
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).
\end{verbatim}

Plot for how the prior compares to the posterior ?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{xdat <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{x=} \KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =}\DecValTok{0}\NormalTok{, }\DataTypeTok{to=}\DecValTok{6}\NormalTok{, }\DataTypeTok{by =}\FloatTok{0.1}\NormalTok{)))}
  \KeywordTok{ggplot}\NormalTok{(xdat) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_ribbon}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{ymin =} \DecValTok{0}\NormalTok{, }\DataTypeTok{ymax =} \KeywordTok{dexp}\NormalTok{(x, }\DataTypeTok{rate =} \DecValTok{1}\NormalTok{)), }
              \DataTypeTok{fill =} \StringTok{"orange2"}\NormalTok{, }\DataTypeTok{alpha =} \DecValTok{1}\OperatorTok{/}\DecValTok{3}\NormalTok{) }\OperatorTok{+}\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{posterior_samples}\NormalTok{(b1.2e),}
               \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ sd_tank__Intercept), }
               \DataTypeTok{fill =} \StringTok{"orange2"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{0}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{breaks =} \OtherTok{NULL}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{coord_cartesian}\NormalTok{(}\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{5}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Prior/posterior plot for sd_tank_intercept"}\NormalTok{, }
       \DataTypeTok{subtitle =} \StringTok{"The prior is the semitransparent ramp in the }\CharTok{\textbackslash{}n}\StringTok{background. The posterior is the solid orange }\CharTok{\textbackslash{}n}\StringTok{mound"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{theme_fivethirtyeight}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Hierarchical_notes_files/figure-latex/unnamed-chunk-16-1.pdf}

Varying intercepts are just regularized estimates, but adaptively
regularized by estimating how diverse the clusters are while estimating
the features of each cluster.

There are three perspectives here

\begin{itemize}
\item{Complete pooling (a single-$\alpha$ model)}
\item{No pooling (the single level $\alpha_{\text{tank}_i}$)}
\item{Partial pooling (the multilevel model for which $\alpha_{\text{tank}}\sim \text{Normal}(\alpha, \sigma)$)}
\end{itemize}

Next section is going to demonstrate the different estimates from the
three models above.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a <-}\StringTok{ }\FloatTok{1.4}
\NormalTok{sigma <-}\StringTok{ }\FloatTok{1.5}
\NormalTok{nponds <-}\StringTok{ }\DecValTok{60}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{12}\NormalTok{)}
\NormalTok{dsim <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{pong =} \DecValTok{1}\OperatorTok{:}\NormalTok{nponds, }
               \DataTypeTok{ni =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{35}\NormalTok{), }\DataTypeTok{each =}\NormalTok{ nponds}\OperatorTok{/}\DecValTok{4}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.integer}\NormalTok{(),}
                        \DataTypeTok{true_a =} \KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =}\NormalTok{ nponds, }\DataTypeTok{mean =}\NormalTok{ a, }\DataTypeTok{sd =}\NormalTok{ sigma))}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{12}\NormalTok{)}
\NormalTok{dsim <-}\StringTok{ }\NormalTok{dsim }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{si =} \KeywordTok{rbinom}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{n}\NormalTok{(), }\DataTypeTok{prob =} \KeywordTok{inv_logit_scaled}\NormalTok{(true_a), }\DataTypeTok{size =}\NormalTok{ ni))}

\KeywordTok{glimpse}\NormalTok{(dsim)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Observations: 60
## Variables: 4
## $ pong   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, ...
## $ ni     <int> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10...
## $ true_a <dbl> -0.82085139, 3.76575421, -0.03511672, 0.01999213, -1.59...
## $ si     <int> 0, 5, 4, 3, 0, 5, 5, 3, 5, 5, 3, 3, 3, 4, 4, 6, 10, 9, ...
\end{verbatim}

Generating no-pooling estiamtes i.e. \(\alpha_{\text{tank}_i}\). This is
the result of simple algebra.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dsim <-}\StringTok{ }\NormalTok{dsim }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{p_nopool =}\NormalTok{ si}\OperatorTok{/}\NormalTok{ni)}

\KeywordTok{glimpse}\NormalTok{(dsim)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Observations: 60
## Variables: 5
## $ pong     <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16...
## $ ni       <int> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10, ...
## $ true_a   <dbl> -0.82085139, 3.76575421, -0.03511672, 0.01999213, -1....
## $ si       <int> 0, 5, 4, 3, 0, 5, 5, 3, 5, 5, 3, 3, 3, 4, 4, 6, 10, 9...
## $ p_nopool <dbl> 0.0, 1.0, 0.8, 0.6, 0.0, 1.0, 1.0, 0.6, 1.0, 1.0, 0.6...
\end{verbatim}

These are the same no-pooling estiamtes you'd get by fitting a model
with a dummy variable (categorical variable) for each pong and flat
priors that induce no regularization.

Generating partial-pooling estimates i.e. \(\alpha_{\text{tank}}\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b1.}\DecValTok{3}\NormalTok{ <-}\StringTok{ }\KeywordTok{brm}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dsim, }\DataTypeTok{family =}\NormalTok{ binomial,}
\NormalTok{            si}\OperatorTok{|}\KeywordTok{trials}\NormalTok{(ni) }\OperatorTok{~}\StringTok{ }\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{|}\NormalTok{pong),}
            \DataTypeTok{prior =} \KeywordTok{c}\NormalTok{(}\KeywordTok{prior}\NormalTok{(}\KeywordTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{class =}\NormalTok{ Intercept),}
                      \KeywordTok{prior}\NormalTok{(}\KeywordTok{cauchy}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{class =}\NormalTok{ sd)), }
            \DataTypeTok{iter =} \DecValTok{10000}\NormalTok{, }\DataTypeTok{warmup =} \DecValTok{1000}\NormalTok{, }\DataTypeTok{chains =} \DecValTok{1}\NormalTok{, }\DataTypeTok{cores =}\NormalTok{ parallel}\OperatorTok{::}\KeywordTok{detectCores}\NormalTok{(),}
            \DataTypeTok{seed =} \DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Compiling the C++ model
\end{verbatim}

\begin{verbatim}
## recompiling to avoid crashing R session
\end{verbatim}

\begin{verbatim}
## Start sampling
\end{verbatim}

\begin{verbatim}
## 
## SAMPLING FOR MODEL '75ee2dac3df591109ee72d37c1f574b6' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 4e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.4 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 10000 [  0%]  (Warmup)
## Chain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)
## Chain 1: Iteration: 1001 / 10000 [ 10%]  (Sampling)
## Chain 1: Iteration: 2000 / 10000 [ 20%]  (Sampling)
## Chain 1: Iteration: 3000 / 10000 [ 30%]  (Sampling)
## Chain 1: Iteration: 4000 / 10000 [ 40%]  (Sampling)
## Chain 1: Iteration: 5000 / 10000 [ 50%]  (Sampling)
## Chain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)
## Chain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)
## Chain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)
## Chain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)
## Chain 1: Iteration: 10000 / 10000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.426824 seconds (Warm-up)
## Chain 1:                2.81924 seconds (Sampling)
## Chain 1:                3.24606 seconds (Total)
## Chain 1:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(b1.}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Family: binomial 
##   Links: mu = logit 
## Formula: si | trials(ni) ~ 1 + (1 | pong) 
##    Data: dsim (Number of observations: 60) 
## Samples: 1 chains, each with iter = 10000; warmup = 1000; thin = 1;
##          total post-warmup samples = 9000
## 
## Group-Level Effects: 
## ~pong (Number of levels: 60) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     1.30      0.18     0.98     1.69 1.00     3075     5663
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.28      0.19     0.91     1.66 1.00     3101     4889
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coef}\NormalTok{(b1.}\DecValTok{3}\NormalTok{)}\OperatorTok{$}\NormalTok{pong[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{, }\DecValTok{59}\OperatorTok{:}\DecValTok{60}\NormalTok{), ,  ] }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{round}\NormalTok{(}\DataTypeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Estimate Est.Error  Q2.5 Q97.5
## 1     -1.07      0.87 -2.94  0.52
## 2      2.30      1.01  0.54  4.46
## 59     0.97      0.36  0.28  1.73
## 60     1.41      0.40  0.67  2.25
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#rhat(mod) extracts rhat }
\CommentTok{#neff_ratio(mode) returns ratios of the effective samples over the total number}
\CommentTok{#of post-warmup iterations. }

\CommentTok{#there is a way to get effective samples (check book)}
\end{Highlighting}
\end{Shaded}

A plot for an estimate of error by model type

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p_partpool <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(b1.}\DecValTok{3}\NormalTok{)}\OperatorTok{$}\NormalTok{pong[, ,] }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{as_tibble}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{transmute}\NormalTok{(}\DataTypeTok{p_partpool =} \KeywordTok{inv_logit_scaled}\NormalTok{(Estimate))}

\NormalTok{dsim <-}\StringTok{ }\NormalTok{dsim }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{bind_cols}\NormalTok{(p_partpool) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{p_true =} \KeywordTok{inv_logit_scaled}\NormalTok{(true_a)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{nopool_error =} \KeywordTok{abs}\NormalTok{(p_nopool }\OperatorTok{-}\StringTok{ }\NormalTok{p_true), }
         \DataTypeTok{partpool_error =} \KeywordTok{abs}\NormalTok{(p_partpool }\OperatorTok{-}\StringTok{ }\NormalTok{p_true))}

\KeywordTok{glimpse}\NormalTok{(dsim)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Observations: 60
## Variables: 9
## $ pong           <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ...
## $ ni             <int> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10...
## $ true_a         <dbl> -0.82085139, 3.76575421, -0.03511672, 0.0199921...
## $ si             <int> 0, 5, 4, 3, 0, 5, 5, 3, 5, 5, 3, 3, 3, 4, 4, 6,...
## $ p_nopool       <dbl> 0.0, 1.0, 0.8, 0.6, 0.0, 1.0, 1.0, 0.6, 1.0, 1....
## $ p_partpool     <dbl> 0.2556068, 0.9085976, 0.8105296, 0.6808351, 0.2...
## $ p_true         <dbl> 0.3055830, 0.9773737, 0.4912217, 0.5049979, 0.1...
## $ nopool_error   <dbl> 0.305582963, 0.022626343, 0.308778278, 0.095002...
## $ partpool_error <dbl> 0.049976191, 0.068776008, 0.319307858, 0.175837...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dfline <-}\StringTok{ }
\StringTok{  }\NormalTok{dsim }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(ni, nopool_error}\OperatorTok{:}\NormalTok{partpool_error) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(key, value, }\OperatorTok{-}\NormalTok{ni) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(key, ni) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean_error =} \KeywordTok{mean}\NormalTok{(value)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{x =}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{31}\NormalTok{, }\DecValTok{46}\NormalTok{),}
         \DataTypeTok{xend =} \KeywordTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{60}\NormalTok{))}

\CommentTok{#calculating mean error for not pooled and partial pooled for each group (n).}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dsim }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ pong)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_vline}\NormalTok{(}\DataTypeTok{xintercept =} \KeywordTok{c}\NormalTok{(}\FloatTok{15.5}\NormalTok{, }\FloatTok{30.5}\NormalTok{, }\FloatTok{45.4}\NormalTok{),}
             \DataTypeTok{color =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{2}\OperatorTok{/}\DecValTok{3}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ nopool_error), }\DataTypeTok{color =} \StringTok{"orange2"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ partpool_error), }\DataTypeTok{shape =}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_segment}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dfline, }
               \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{xend =}\NormalTok{ xend, }
               \DataTypeTok{y =}\NormalTok{ mean_error, }\DataTypeTok{yend =}\NormalTok{ mean_error),}
               \DataTypeTok{color =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"orange2"}\NormalTok{, }\StringTok{"black"}\NormalTok{), }\DataTypeTok{each=}\DecValTok{4}\NormalTok{),}
               \DataTypeTok{linetype =} \KeywordTok{rep}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{, }\DataTypeTok{each =} \DecValTok{4}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_continuous}\NormalTok{(}\DataTypeTok{breaks =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{60}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{, }\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\DecValTok{15} \OperatorTok{-}\StringTok{ }\FloatTok{7.5}\NormalTok{, }\DecValTok{30}\OperatorTok{-}\StringTok{ }\FloatTok{7.5}\NormalTok{, }\DecValTok{45} \OperatorTok{-}\StringTok{ }\FloatTok{7.5}\NormalTok{, }\DecValTok{60} \OperatorTok{-}\StringTok{ }\FloatTok{7.5}\NormalTok{), }\DataTypeTok{y =} \FloatTok{0.45}\NormalTok{,}
           \DataTypeTok{label =} \KeywordTok{c}\NormalTok{(}\StringTok{"tiny (5)"}\NormalTok{, }\StringTok{"small (10)"}\NormalTok{, }\StringTok{"medium (25)"}\NormalTok{, }\StringTok{"large (35)"}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{y =} \StringTok{"absolute error"}\NormalTok{, }
       \DataTypeTok{title =} \StringTok{"Estimate error by model type"}\NormalTok{, }
       \DataTypeTok{subtitle =} \StringTok{"The horizontal axis displays pond number. The vertical axis measures}\CharTok{\textbackslash{}n}\StringTok{the absolute error in the predicted proportion of survivors, compared to}\CharTok{\textbackslash{}n}\StringTok{the true value used in the simulation. The higher the point, the worse}\CharTok{\textbackslash{}n}\StringTok{the estimate. No-pooling shown in orange. Partial pooling shown in black.}\CharTok{\textbackslash{}n}\StringTok{The orange and dashed black lines show the average error for each kind}\CharTok{\textbackslash{}n}\StringTok{of estimate, across each initial density of tadpoles (pond size). Smaller}\CharTok{\textbackslash{}n}\StringTok{ponds produce more error, but the partial pooling estimates are better}\CharTok{\textbackslash{}n}\StringTok{on average, especially in smaller ponds."}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{theme_fivethirtyeight}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{panel.grid =} \KeywordTok{element_blank}\NormalTok{(),}
        \DataTypeTok{plot.subtitle =}  \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Hierarchical_notes_files/figure-latex/unnamed-chunk-23-1.pdf}

A simple quantitative summary of error might be,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dsim }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(ni, nopool_error}\OperatorTok{:}\NormalTok{partpool_error) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(key, value, }\OperatorTok{-}\NormalTok{ni) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(key) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean_error   =} \KeywordTok{mean}\NormalTok{(value) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{round}\NormalTok{(}\DataTypeTok{digits =} \DecValTok{3}\NormalTok{),}
            \DataTypeTok{median_error =} \KeywordTok{median}\NormalTok{(value) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{round}\NormalTok{(}\DataTypeTok{digits =} \DecValTok{3}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 3
##   key            mean_error median_error
##   <chr>               <dbl>        <dbl>
## 1 nopool_error        0.078        0.05 
## 2 partpool_error      0.067        0.051
\end{verbatim}

\subsection{13.2 Chimpanzee}\label{chimpanzee}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chimp <-}\StringTok{ }\KeywordTok{readRDS}\NormalTok{(}\StringTok{"~/Documents/R/STAT610/STA610_Labs/Oct30/chimpanzees.Rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The model we use for this dataset is

\[
\begin{aligned}
\text{left_pull}_i &\sim \text{Binomial}(n_i = 1, p_i)\\
\text{logit}(p_i) &= \alpha + \alpha_{\text{actor}_i} + (\beta_1 + \beta_2\text{condition}_i)\text{prosoc_left}_i\\
\alpha_{\text{actor}} &\sim \text{Normal}(0, \sigma_{\text{actor}})\\
\alpha &\sim \text{Normal}(0, 10)\\
\beta_1 &\sim \text{Normal}(0, 10)\\
\beta_2 &\sim \text{Normal}(0, 10)\\
\sigma_{\text{actor}} &\sim \text{HalfCauchy}(0, 1)\\
\end{aligned}
\]

Here, the mean of actor \(\alpha\) is taken out of the random effect and
treated as a constant that each random effect group i will add or
subtract from.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b2.}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{brm}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ chimp, }\DataTypeTok{family =}\NormalTok{ binomial, }
\NormalTok{            pulled_left }\OperatorTok{|}\KeywordTok{trials}\NormalTok{(}\DecValTok{1}\NormalTok{) }\OperatorTok{~}\StringTok{ }\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{prosoc_left }\OperatorTok{+}\StringTok{ }\NormalTok{prosoc_left}\OperatorTok{:}\NormalTok{condition }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{|}\NormalTok{actor),}
            \DataTypeTok{prior =} \KeywordTok{c}\NormalTok{(}\KeywordTok{prior}\NormalTok{(}\KeywordTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{), }\DataTypeTok{class =}\NormalTok{ Intercept),}
                      \KeywordTok{prior}\NormalTok{(}\KeywordTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{), }\DataTypeTok{class =}\NormalTok{ b), }
                      \KeywordTok{prior}\NormalTok{(}\KeywordTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{), }\DataTypeTok{class =}\NormalTok{ sd)),}
            \DataTypeTok{iter =} \DecValTok{5000}\NormalTok{, }\DataTypeTok{warmup =} \DecValTok{1000}\NormalTok{, }\DataTypeTok{chains =} \DecValTok{4}\NormalTok{, }\DataTypeTok{cores =}\NormalTok{ parallel}\OperatorTok{::}\KeywordTok{detectCores}\NormalTok{(),}
            \DataTypeTok{control =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{adapt_delta =} \FloatTok{0.95}\NormalTok{), }
            \DataTypeTok{seed =} \DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Only 2 levels detected so that family 'bernoulli' might be a more efficient choice.
\end{verbatim}

\begin{verbatim}
## Compiling the C++ model
\end{verbatim}

\begin{verbatim}
## Start sampling
\end{verbatim}

\begin{verbatim}
## Warning: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.95 may help. See
## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
\end{verbatim}

\begin{verbatim}
## Warning: Examine the pairs() plot to diagnose sampling problems
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(b2.}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.95 may help.
## See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
\end{verbatim}

\begin{verbatim}
##  Family: binomial 
##   Links: mu = logit 
## Formula: pulled_left | trials(1) ~ 1 + prosoc_left + prosoc_left:condition + (1 | actor) 
##    Data: chimp (Number of observations: 504) 
## Samples: 4 chains, each with iter = 5000; warmup = 1000; thin = 1;
##          total post-warmup samples = 16000
## 
## Group-Level Effects: 
## ~actor (Number of levels: 7) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     2.97      1.46     1.28     6.87 1.00     2597     3192
## 
## Population-Level Effects: 
##                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## Intercept                 0.54      1.27    -1.88     3.20 1.00     2885
## prosoc_left               0.82      0.26     0.32     1.33 1.00     8055
## prosoc_left:condition    -0.13      0.30    -0.71     0.46 1.00     6823
##                       Tail_ESS
## Intercept                 3083
## prosoc_left               9424
## prosoc_left:condition     7784
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).
\end{verbatim}

Next, add the actor-level deviations to the fixed intercept i.e.~the
grand mean.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{post <-}\StringTok{ }\KeywordTok{posterior_samples}\NormalTok{(b2.}\DecValTok{1}\NormalTok{)}

\NormalTok{post }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{starts_with}\NormalTok{(}\StringTok{"r_actor"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{value =}\NormalTok{ value }\OperatorTok{+}\StringTok{ }\NormalTok{post}\OperatorTok{$}\NormalTok{b_Intercept) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(key) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(value) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{round}\NormalTok{(}\DataTypeTok{digits =} \DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 7 x 2
##   key                   mean
##   <chr>                <dbl>
## 1 r_actor[1,Intercept] -0.72
## 2 r_actor[2,Intercept]  5.27
## 3 r_actor[3,Intercept] -1.03
## 4 r_actor[4,Intercept] -1.02
## 5 r_actor[5,Intercept] -0.72
## 6 r_actor[6,Intercept]  0.23
## 7 r_actor[7,Intercept]  1.78
\end{verbatim}

Another way to get the same information plus 95\% confidence intervals

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coef}\NormalTok{(b2.}\DecValTok{1}\NormalTok{)}\OperatorTok{$}\NormalTok{actor[, }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\OperatorTok{:}\DecValTok{4}\NormalTok{), }\DecValTok{1}\NormalTok{] }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{as_tibble}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{round}\NormalTok{(}\DataTypeTok{digits =} \DecValTok{2}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# here we put the credible intervals in an APA-6-style format}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\StringTok{`}\DataTypeTok{95% CIs}\StringTok{`}\NormalTok{ =}\StringTok{ }\KeywordTok{str_c}\NormalTok{(}\StringTok{"["}\NormalTok{, Q2.}\DecValTok{5}\NormalTok{, }\StringTok{", "}\NormalTok{, Q97.}\DecValTok{5}\NormalTok{, }\StringTok{"]"}\NormalTok{),}
         \DataTypeTok{actor     =} \KeywordTok{str_c}\NormalTok{(}\StringTok{"chimp #"}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{7}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{mean =}\NormalTok{ Estimate) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(actor, mean, }\StringTok{`}\DataTypeTok{95% CIs}\StringTok{`}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrl@{}}
\toprule
actor & mean & 95\% CIs\tabularnewline
\midrule
\endhead
chimp \#1 & -0.72 & {[}-1.25, -0.2{]}\tabularnewline
chimp \#2 & 5.27 & {[}2.71, 10.96{]}\tabularnewline
chimp \#3 & -1.03 & {[}-1.58, -0.48{]}\tabularnewline
chimp \#4 & -1.02 & {[}-1.58, -0.49{]}\tabularnewline
chimp \#5 & -0.72 & {[}-1.26, -0.2{]}\tabularnewline
chimp \#6 & 0.23 & {[}-0.3, 0.76{]}\tabularnewline
chimp \#7 & 1.78 & {[}1.06, 2.59{]}\tabularnewline
\bottomrule
\end{longtable}

Let's add a block effect to the model

\[
\begin{aligned}
\text{left_pull}_i &\sim \text{Binomial}(n_i = 1, p_i)\\
\text{logit}(p_i) &= \alpha + \alpha_{\text{actor}_i} + \alpha_{\text{block}_i} + (\beta_1 + \beta_2\text{condition}_i)\text{prosoc_left}_i\\
\alpha_{\text{actor}} &\sim \text{Normal}(0, \sigma_{\text{actor}})\\
\alpha_{\text{block}} &\sim \text{Normal}(0, \sigma_{\text{block}})\\
\alpha &\sim \text{Normal}(0, 10)\\
\beta_1 &\sim \text{Normal}(0, 10)\\
\beta_2 &\sim \text{Normal}(0, 10)\\
\sigma_{\text{actor}} &\sim \text{HalfCauchy}(0, 1)\\
\sigma_{\text{block}} &\sim \text{HalfCauchy}(0,1)\\
\end{aligned}
\]

This model now has varying intercepts for both actor and block

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b2.}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{brm}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ chimp, }\DataTypeTok{family =}\NormalTok{ binomial, }
\NormalTok{            pulled_left}\OperatorTok{|}\KeywordTok{trials}\NormalTok{(}\DecValTok{1}\NormalTok{) }\OperatorTok{~}\StringTok{ }\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{|}\NormalTok{actor) }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{|}\NormalTok{block) }\OperatorTok{+}\StringTok{ }
\StringTok{              }\NormalTok{prosoc_left }\OperatorTok{+}\StringTok{ }\NormalTok{prosoc_left}\OperatorTok{:}\NormalTok{condition,}
            \DataTypeTok{prior =} \KeywordTok{c}\NormalTok{(}\KeywordTok{prior}\NormalTok{(}\KeywordTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{), }\DataTypeTok{class =}\NormalTok{ Intercept),}
                      \KeywordTok{prior}\NormalTok{(}\KeywordTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{), }\DataTypeTok{class =}\NormalTok{ b), }
                      \KeywordTok{prior}\NormalTok{(}\KeywordTok{cauchy}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{class =}\NormalTok{ sd)), }
            \DataTypeTok{iter =} \DecValTok{5000}\NormalTok{, }\DataTypeTok{warmup =}\DecValTok{1000}\NormalTok{, }\DataTypeTok{chains =} \DecValTok{4}\NormalTok{, }
            \DataTypeTok{cores =}\NormalTok{ parallel}\OperatorTok{::}\KeywordTok{detectCores}\NormalTok{(), }
            \DataTypeTok{control =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{adapt_delta =} \FloatTok{0.95}\NormalTok{), }
            \DataTypeTok{seed =} \DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Only 2 levels detected so that family 'bernoulli' might be a more efficient choice.
\end{verbatim}

\begin{verbatim}
## Compiling the C++ model
\end{verbatim}

\begin{verbatim}
## Start sampling
\end{verbatim}

\begin{verbatim}
## Warning: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.95 may help. See
## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
\end{verbatim}

\begin{verbatim}
## Warning: Examine the pairs() plot to diagnose sampling problems
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#increase adapt_delta to 0.99 to reduce/avoid divergent transitions}

\KeywordTok{summary}\NormalTok{(b2.}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.95 may help.
## See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
\end{verbatim}

\begin{verbatim}
##  Family: binomial 
##   Links: mu = logit 
## Formula: pulled_left | trials(1) ~ 1 + (1 | actor) + (1 | block) + prosoc_left + prosoc_left:condition 
##    Data: chimp (Number of observations: 504) 
## Samples: 4 chains, each with iter = 5000; warmup = 1000; thin = 1;
##          total post-warmup samples = 16000
## 
## Group-Level Effects: 
## ~actor (Number of levels: 7) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     2.28      0.92     1.13     4.64 1.00     4510     7373
## 
## ~block (Number of levels: 6) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.22      0.18     0.01     0.66 1.00     5581     5971
## 
## Population-Level Effects: 
##                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## Intercept                 0.43      0.95    -1.39     2.38 1.00     4131
## prosoc_left               0.82      0.26     0.32     1.33 1.00    14852
## prosoc_left:condition    -0.14      0.30    -0.72     0.45 1.00    15034
##                       Tail_ESS
## Intercept                 5201
## prosoc_left              12404
## prosoc_left:condition    11855
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(bayesplot)}

\NormalTok{post <-}\StringTok{ }\KeywordTok{posterior_samples}\NormalTok{(b2.}\DecValTok{2}\NormalTok{, }\DataTypeTok{add_chain =}\NormalTok{ T)}

\NormalTok{post }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{lp__, }\OperatorTok{-}\NormalTok{iter) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mcmc_trace}\NormalTok{(}\DataTypeTok{facet_args =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{ncol =} \DecValTok{4}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_x_continuous}\NormalTok{(}\DataTypeTok{breaks =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{2500}\NormalTok{, }\DecValTok{5000}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{theme_fivethirtyeight}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \KeywordTok{c}\NormalTok{(.}\DecValTok{75}\NormalTok{, .}\DecValTok{06}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Hierarchical_notes_files/figure-latex/unnamed-chunk-30-1.pdf}

Let's look at the \(n_{eff}/N\) ratios

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{color_scheme_set}\NormalTok{(}\StringTok{"orange"}\NormalTok{)}

\KeywordTok{neff_ratio}\NormalTok{(b2.}\DecValTok{2}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mcmc_neff}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{theme_fivethirtyeight}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Hierarchical_notes_files/figure-latex/unnamed-chunk-31-1.pdf}

Some values are lower than what is good. But none are below 0.1.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ranef}\NormalTok{(b2.}\DecValTok{2}\NormalTok{)}\OperatorTok{$}\NormalTok{actor[, , }\StringTok{"Intercept"}\NormalTok{] }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{round}\NormalTok{(}\DataTypeTok{digits =} \DecValTok{2}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rrrr@{}}
\toprule
Estimate & Est.Error & Q2.5 & Q97.5\tabularnewline
\midrule
\endhead
-1.15 & 0.96 & -3.13 & 0.68\tabularnewline
4.21 & 1.66 & 1.80 & 8.17\tabularnewline
-1.46 & 0.97 & -3.47 & 0.37\tabularnewline
-1.46 & 0.96 & -3.45 & 0.38\tabularnewline
-1.15 & 0.96 & -3.12 & 0.68\tabularnewline
-0.20 & 0.96 & -2.18 & 1.66\tabularnewline
1.34 & 0.98 & -0.65 & 3.25\tabularnewline
\bottomrule
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ranef}\NormalTok{(b2.}\DecValTok{2}\NormalTok{)}\OperatorTok{$}\NormalTok{block[, , }\StringTok{"Intercept"}\NormalTok{] }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{round}\NormalTok{(}\DataTypeTok{digits =} \DecValTok{2}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rrrr@{}}
\toprule
Estimate & Est.Error & Q2.5 & Q97.5\tabularnewline
\midrule
\endhead
-0.17 & 0.23 & -0.73 & 0.13\tabularnewline
0.04 & 0.19 & -0.34 & 0.47\tabularnewline
0.05 & 0.19 & -0.29 & 0.49\tabularnewline
0.01 & 0.19 & -0.38 & 0.41\tabularnewline
-0.03 & 0.19 & -0.45 & 0.33\tabularnewline
0.11 & 0.20 & -0.21 & 0.59\tabularnewline
\bottomrule
\end{longtable}

Coefficient plot

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{brms}\OperatorTok{::}\KeywordTok{stanplot}\NormalTok{(b2.}\DecValTok{2}\NormalTok{, }\DataTypeTok{pars =} \KeywordTok{c}\NormalTok{(}\StringTok{"^r_"}\NormalTok{, }\StringTok{"^b_"}\NormalTok{, }\StringTok{"^sd_"}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{theme_fivethirtyeight}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.y =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{hjust =} \DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Hierarchical_notes_files/figure-latex/unnamed-chunk-34-1.pdf}

We can also look at the posterior samples as it's easy to compare the
random variances

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{post }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ sd_actor__Intercept)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{size =} \DecValTok{0}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"orange1"}\NormalTok{, }\DataTypeTok{alpha =} \DecValTok{3}\OperatorTok{/}\DecValTok{4}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{ sd_block__Intercept), }
               \DataTypeTok{size =} \DecValTok{0}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"orange4"}\NormalTok{, }\DataTypeTok{alpha =} \DecValTok{3}\OperatorTok{/}\DecValTok{4}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{breaks =} \OtherTok{NULL}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_cartesian}\NormalTok{(}\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \KeywordTok{expression}\NormalTok{(sigma)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{, }\DataTypeTok{x =} \DecValTok{2}\OperatorTok{/}\DecValTok{3}\NormalTok{, }\DataTypeTok{y =} \DecValTok{2}\NormalTok{, }\DataTypeTok{label =} \StringTok{"block"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"orange4"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{, }\DataTypeTok{x =} \DecValTok{2}\NormalTok{, }\DataTypeTok{y =} \DecValTok{3}\OperatorTok{/}\DecValTok{4}\NormalTok{, }\DataTypeTok{label =} \StringTok{"actor"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"orange1"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{theme_fivethirtyeight}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Hierarchical_notes_files/figure-latex/unnamed-chunk-35-1.pdf}

It looks like the block random effect is centered nearly at zero,
indicating that the blocks have almost no variation within them, whereas
actor standard deviation is centered around 2 and has a large standard
deviation indicating that actors are highly variable and have more
variation among them.

We can do a model selection using their LOO values

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b2.}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{add_criterion}\NormalTok{(b2.}\DecValTok{1}\NormalTok{, }\StringTok{"loo"}\NormalTok{)}
\NormalTok{b2.}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{add_criterion}\NormalTok{(b2.}\DecValTok{2}\NormalTok{, }\StringTok{"loo"}\NormalTok{)}

\KeywordTok{loo_compare}\NormalTok{(b2.}\DecValTok{1}\NormalTok{, b2.}\DecValTok{2}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{(}\DataTypeTok{simplfy =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      elpd_diff se_diff
## b2.1  0.0       0.0   
## b2.2 -0.8       0.8
\end{verbatim}

\section{Appendix}\label{appendix}

\subsection{1A. Stuff Shubhi Forgets - Correlation, Covariance,
Variance}\label{a.-stuff-shubhi-forgets---correlation-covariance-variance}

Correlation (r) is a statistical tool used to assess the degree of
association of two quantitative variables measured in each member of a
group.

\[
Cor(x, y) = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum(x_i - \bar{x})^2 \sum(y_i - \bar{y})^2}}
\]

or,

\[
r = \frac{cov(x,y)}{\sigma_x \sigma_y}
\]

The square of the correlation coefficient is \(R^2\), or the coefficient
of determination. Covariance is the variation of x and y together,

\[Cov(x, y) = E[(x_i - E[x])(y_i- E[y])]\]

and finally, variance is the variaiton in x,

\[Var(x) = E[(x_i - E[x])^2]\]


\end{document}
